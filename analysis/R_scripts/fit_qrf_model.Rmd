---
title: "QRF Covariate Selection"
author: "Kevin See"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: html_notebook
editor_options: 
  chunk_output_type: console
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
library(knitr)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)

# load packages for analysis
library(QRFcapacity)
library(maptools)
library(tidyverse)
library(janitor)
library(magrittr)
library(minerva)
library(sf)
library(quantregForest)
library(survey)
library(MuMIn)


# set default theme for ggplot
theme_set(theme_bw())
# setwd('analysis/R_scripts')
```

# Select QRF Habitat Covariates

We loaded the data that had been prepped and saved as part of the `QRFcapacity` package. We used the data from all CHaMP sites, 2011-2017.

```{r load-data}
# determine which set of fish/habitat data to use
data("fh_sum_champ_2017")
fish_hab = fh_sum_champ_2017 %>%
  filter(Species == 'Chinook') %>%
  mutate_at(vars(Watershed, Year),
            list(as.factor))
  

# and the appropriate habitat dictionrary to go with it
data("hab_dict_2017")
hab_dict = hab_dict_2017

# all the related habitat data
data("champ_site_2011_17")
hab_data = champ_site_2011_17 %>%
  filter(!Watershed %in% c('Big-Navarro-Garcia (CA)',
                           'CHaMP Training',
                           'Region 17',
                           'Walla Walla',
                           'Umatilla'))

data("champ_site_2011_17_avg")
hab_avg = champ_site_2011_17_avg %>%
  filter(!Watershed %in% c('Big-Navarro-Garcia (CA)',
                           'CHaMP Training',
                           'Region 17',
                           'Walla Walla',
                           'Umatilla'))

# add temperature metrics
data("champ_temps")

# add average aug temperature to average CHaMP metrics
hab_avg %<>%
  left_join(champ_temps %>%
              as_tibble() %>%
              select(Site, avg_aug_temp = S2_02_11) %>%
              distinct())

# add specific years' temperature, and average for each site
hab_data %<>%
  left_join(hab_data %>%
              select(VisitID, Year = VisitYear) %>%
              distinct() %>%
              left_join(champ_temps %>%
                          as_tibble() %>%
                          select(VisitID, avg_aug_temp = S2_02_11))) %>%
  left_join(hab_data %>%
              select(VisitID, Year = VisitYear) %>%
              distinct() %>%
              left_join(champ_temps %>%
                          as_tibble() %>%
                          select(Site:VisitID, S1_93_11:S36_2015) %>%
                          gather(scenario, aug_temp, S1_93_11:S36_2015) %>%
                          mutate(Year = str_sub(scenario, -4)) %>%
                          mutate_at(vars(Year),
                                    list(as.numeric)) %>%
                          filter(!is.na(Year)) %>%
                          select(Site:VisitID, Year, aug_temp))) %>%
  select(-Year)

```

All sites fell within the range of steelhead, but many of them were outside the range of spring/summer Chinook salmon. However, defining the range of sp/su Chinook is tricky, and it is unclear if the reported data for Chinook salmon includes only sites considered with the range of Chinook, or if we should exclude data from some sites based on our best understanding of the species' range. Currently, we are excluding sites outside the Chinook domain. We created our own list of sites in the John Day that were within a Chinook domain, based on which sites Chinook were found at some point (and downstream of those).

```{r chnk-domain-filter}
data("chnk_domain")

# which sites were sampled for Chinook? 
chnk_samps = fish_hab %>%
  filter(Species == 'Chinook') %>%
  select(Site:Lon, N) %>%
  distinct() %>%
  st_as_sf(coords = c('Lon', 'Lat'),
           crs = 4326) %>%
  st_transform(st_crs(chnk_domain))

# set snap distance (in meters)
st_crs(chnk_samps)
max_snap_dist = 1000

# which of those sites are in Chinook domain?
chnk_sites = chnk_samps %>%
  as_Spatial() %>%
  snapPointsToLines_v2(points = .,
  # maptools::snapPointsToLines(points = .,
                              lines = chnk_domain %>%
                                mutate(id = 1:n()) %>%
                                select(id, MPG) %>%
                                as_Spatial(),
                              maxDist = max_snap_dist,
                              withAttrs = TRUE,
                              idField = 'id') %>%
  as('sf') %>%
  select(-nearest_line_id, -snap_dist) %>%
  # include some sites in the John Day where Chinook were found (or seemed to be close to sites where Chinook were found)
  rbind(st_read('../data/derived_data/Chnk_JohnDay_TrueObs.shp',
                quiet = T) %>%
          st_transform(st_crs(chnk_domain)) %>%
          select(-in_range))

fish_hab %<>%
  filter(Site %in% chnk_sites$Site)
  # filter(Species == 'Steelhead' |
  #          (Species == 'Chinook' & Site %in% chnk_sites$Site))
  
```

Next we generated MINE stats for all possible habitat metrics, matching them up against the log of fish densities (plus a small value to deal with 0s).

```{r MINE_stats}

# # what are some possible habitat covariates?
poss_hab_mets = hab_dict %>%
  filter(MetricCategory != 'Categorical') %>%
  filter(ShortName %in% names(fish_hab)) %>%
  pull(ShortName) %>%
  unique()

# poss_hab_mets = c(poss_hab_mets, 'aug_temp', 'avg_aug_temp') %>%
#   unique()


mine_res = fish_hab %>%
  mutate_at(vars(starts_with('LWVol')),
            list(~ . / Lgth_Wet)) %>%
  mutate(fish_dens = log(fish_dens + 0.005)) %>%
  estimate_MIC(covars = poss_hab_mets,
               response = 'fish_dens') %>%
  mutate(Species = 'Chinook') %>%
  left_join(hab_dict %>%
              filter(MetricGroupName == 'Visit Metric') %>%
              select(Metric = ShortName,
                     MetricCategory,
                     Name),
            by = 'Metric') %>%
  mutate_at(vars(MetricCategory),
            list(fct_explicit_na),
            na_level = 'Other') %>%
  mutate_at(vars(Name),
            list(as.character)) %>%
  mutate(Name = if_else(is.na(Name),
                        as.character(Metric),
                        Name)) %>%
  # put the metric names in descending order by MIC
  mutate_at(vars(Metric, Name),
            list(~ fct_reorder(., .x = MIC))) %>%
  select(Species, MetricCategory, Metric, everything()) %>%
  arrange(Species, MetricCategory, desc(MIC))

```

Plot the MINE results and save figures for manuscript.

```{r MINE-figures}
mine_plot_df = mine_res %>%
  # filter out some metrics with too many NAs or 0s
  filter((perc_NA < 0.2 & non_0 > 100) | MetricCategory == 'Temperature') %>%
  # filter out metrics with very low variance
  # filter(var < 0.1) %>%
  # filter(obsCV < 0.1)
  # janitor::tabyl(MetricCategory)
  # select(1:11)
  # filter out area and volume metrics
  filter(!grepl('Area$', Metric),
         !grepl('Vol$', Metric),
         !Metric %in% c('Lgth_Wet', 
                        'Lgth_BfChnl',
                        'Lgth_WetChnl',
                        'Area_Wet', 
                        'Area_Bf', 
                        'WetVol', 
                        'BfVol'))

mine_p = mine_plot_df %>%
  ggplot(aes(x = Name,
             y = MIC)) +
  geom_col(position = position_dodge(1),
           fill = 'gray45') +
  coord_flip() +
  facet_wrap(~ MetricCategory,
             scales = 'free_y',
             ncol = 3) +
  scale_fill_brewer(palette = 'Set1',
                    guide = guide_legend(nrow = 1)) +
  theme(legend.position = 'bottom',
        axis.text = element_text(size = 5)) +
  labs(y = 'Habitat Covariate')

mine_p2 = mine_plot_df %>%
  ggplot(aes(x = Name,
             y = MIC,
             fill = MetricCategory)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = 'Set1',
                    guide = guide_legend(nrow = 1)) +
  theme(legend.position = 'bottom',
        axis.text = element_text(size = 5)) +
  labs(y = 'Habitat Covariate')

```

```{r, eval = F}
library(corrr)
#----------------------------------------------
# Look at correlations between habitat metrics
#----------------------------------------------
# top metrics
sel_mets = poss_hab_mets
sel_mets = mine_plot_df %>%
  group_by(MetricCategory) %>%
  slice(1:5) %>%
  ungroup() %>%
  pull(Metric) %>%
  unique() %>%
  as.character()


corr_mat = hab_avg %>%
  select(one_of(sel_mets)) %>%
  corrr::correlate()

corr_mat %>%
  corrr::rearrange(absolute = F) %>%
  corrr::shave(upper = T) %>% 
  corrr::stretch() %>%
  filter(!is.na(r)) %>%
  filter(abs(r) > 0.5) %>%
  kable()

corr_p1 = corr_mat %>%
  # rearrange(absolute = F) %>%
  shave(upper = T) %>% 
  rplot(legend = T,
        print_cor = T)

corr_p2 = network_plot(corr_mat)
```

```{r correlation-plots}
#----------------------------------------------
# Look at correlations between habitat metrics
#----------------------------------------------

library(ggcorrplot)

corr_list = hab_dict %>%
  filter(MetricCategory != 'Categorical') %>%
  split(list(.$MetricCategory)) %>%
  map(.f = function(x) {
    catg_mets = x %>%
      pull(ShortName)
    hab_data %>%
      select(one_of(catg_mets)) %>%
      cor(use = "pairwise.complete.obs")
      # corrr::correlate(use = "pairwise.complete.obs",
      #                  method = "pearson",
      #                  quiet = T)
  })
  
# make correlation plots by metric category
corr_p_list = corr_list %>%
  map(.f = ggcorrplot,
      method = "circle",
      show.diag = F,
      type = 'lower')

# add a title to each plot
for(i in 1:length(corr_p_list)) {
  corr_p_list[[i]] = corr_p_list[[i]] +
    labs(title = names(corr_list)[i])
}

# save as a PDF
pdf("../figures/correlation_plots_all.pdf",
    width = 8,
    height = 8)
for(i in 1:length(corr_p_list)) {
  print(corr_p_list[[i]])
}
dev.off()

```

Based on the MIC results, and trying to stay away from pairs of metrics that were too closely correlated, we came up with the following list of QRF covariates.

```{r habitat-covariates}
sel_hab_mets = tibble(Species = c('Chinook'),
                      Metric = c('UcutArea_Pct',
                                 'FishCovNone',
                                 'SubEstGrvl',
                                 'FstTurb_Freq',
                                 'FstNT_Freq',
                                 'CU_Freq',
                                 'SlowWater_Pct',
                                 'NatPrin1',
                                 'DistPrin1',
                                 'avg_aug_temp',
                                 # 'Sin_CL',
                                 'Sin',
                                 'WetWdth_CV',
                                 'WetBraid',
                                 'WetSC_Pct',
                                 'Q',
                                 'WetWdth_Int',
                                 'LWFreq_Wet',
                                 'LWVol_WetFstTurb'))

# sum(!sel_hab_mets$Metric %in% names(fish_hab))
```

```{r}
sel_hab_mets %>%
  select(ShortName = Metric) %>%
  distinct() %>%
  left_join(hab_dict) %>%
  select(Catg = MetricCategory, 
         Metric = Name, 
         Description = DescriptiveText) %>%
  kable()
```

# Fit QRF Model

```{r}
#-----------------------------------------------------------------
# Fit QRF model
#-----------------------------------------------------------------
covars = sel_hab_mets$Metric
# impute missing data in fish / habitat dataset
set.seed(7)
qrf_data = fish_hab %>%
  impute_missing_data(covars = covars,
                      impute_vars = c('Watershed', 'Elev_M', 'Sin', 'Year', 'CUMDRAINAG'),
                      method = 'missForest') %>%
  select(Site, Watershed, Year, LON_DD, LAT_DD, fish_dens, VisitID, one_of(covars))

# fit the QRF model
# set the density offset (to accommodate 0z)
dens_offset = 0.005

# fit random forest models
set.seed(4)
qrf_mod = quantregForest(x = qrf_data %>%
                               select(one_of(covars)) %>%
                               as.matrix,
                             y = qrf_data %>%
                               mutate_at(vars(fish_dens),
                                         list(~ log(. + dens_offset))) %>%
                               select(fish_dens) %>%
                               as.matrix(),
                             keep.inbag = T,
                             ntree = 1000)

# save some results
# usethis::use_data(fish_hab, 
#                   hab_dict,
#                   hab_data,
#                   hab_avg,
#                   sel_hab_mets,
#                   qrf_data,
#                   dens_offset,
#                   qrf_mod,
#                   version = 2,
#                   overwrite = T)

write_rds(fish_hab,
          "../data/derived_data/fish_hab.rds")
write_rds(hab_dict,
          "../data/derived_data/hab_dict.rds")
write_rds(hab_data,
          "../data/derived_data/hab_data.rds")
write_rds(sel_hab_mets,
          "../data/derived_data/sel_hab_mets.rds")
write_rds(qrf_data,
          "../data/derived_data/qrf_data.rds")
write_rds(dens_offset,
          "../data/derived_data/dens_offset.rds")
write_rds(qrf_mod,
          "../data/derived_data/qrf_mod.rds")

```

## Predict at all CHaMP sites

```{r champ_predictions}
# what quantile is a proxy for capacity?
pred_quant = 0.9

covars = unique(sel_hab_mets$Metric)

# impute missing values in habitat data
hab_impute = hab_avg %>%
  mutate_at(vars(Watershed, Channel_Type),
            list(fct_drop)) %>%
  impute_missing_data(data = .,
                      covars = covars,
                      impute_vars = c('Watershed', 
                                      'Elev_M', 
                                      'Channel_Type', 
                                      'CUMDRAINAG'),
                      method = 'missForest') %>%
  select(Site, Watershed, LON_DD, LAT_DD, VisitYear, Lgth_Wet, Area_Wet, one_of(covars))

# predict at all CHaMP sites
pred_hab_sites = hab_impute %>%
  mutate(chnk_per_m = predict(qrf_mod,
                              newdata = select(., one_of(covars)),
                              what = pred_quant),
         chnk_per_m = exp(chnk_per_m) - dens_offset,
         chnk_per_m2 = chnk_per_m * Lgth_Wet / Area_Wet)

# filter out sites outside the Chinook domain
data("chnk_domain")
snap_dist = 1000

pred_hab_sites_chnk = pred_hab_sites %>%
  filter(!is.na(LON_DD)) %>%
  st_as_sf(coords = c('LON_DD', 'LAT_DD'),
           crs = 4326) %>%
  st_transform(crs = st_crs(chnk_domain)) %>%
  as_Spatial() %>%
  snapPointsToLines_v2(chnk_domain %>%
  # maptools::snapPointsToLines(chnk_domain %>%
                                mutate(id = 1:n()) %>%
                                select(id, MPG) %>%
                                as_Spatial(),
                              maxDist = snap_dist,
                              withAttrs = T,
                              idField = 'id') %>%
  as('sf') %>%
  as_tibble() %>%
  select(-nearest_line_id, -snap_dist, -geometry) %>%
  # add sites in the John Day
  bind_rows(st_read('../data/derived_data/Chnk_JohnDay_TrueObs.shp',
                    quiet = T) %>%
              as_tibble() %>%
              select(Site) %>%
              inner_join(pred_hab_sites))

```

# Extrapolation

Because most of our habitat data came from CHaMP sites, which were selected through a generalized random tesselation stratified (GRTS) sample design [@Stevens2003], we wanted to incorporate the design weights for each CHaMP site with a predicted carrying capacity. 

```{r survey_design_strata}
#----------------------------------------
# pull in survey design related data
#----------------------------------------
# Calculate GRTS design weights.

# pull in info about what strata each CHaMP site was assigned to (using 2014 as reference year)
site_strata = pred_hab_sites_chnk %>%
  select(Site, Watershed) %>%
  distinct() %>%
  left_join(gaa %>%
              select(Site,
                     # CHaMPsheds,
                     strata = AStrat2014)) %>%
  mutate(site_num = str_split(Site, '-', simplify = T)[,2]) %>%
  mutate(strata = if_else(Watershed == 'Asotin',
                          site_num,
                          if_else(Watershed == 'Entiat' & grepl('ENT00001', Site),
                                  paste('EntiatIMW', site_num, sep = '_'),
                                  strata))) %>%
  mutate(strata = if_else(grepl('EntiatIMW', strata),
                          str_remove(strata, '[[:digit:]]$'),
                          strata),
         strata = if_else(grepl('EntiatIMW', strata),
                          str_remove(strata, '[[:digit:]]$'),
                          strata)) %>%
  filter(!is.na(strata)) %>%
  mutate(strata = paste(Watershed, strata, sep = '_')) %>%
  select(-site_num)

# read in data from the CHaMP frame
champ_frame_df = read_csv('../data/derived_data/champ_frame_data.csv') %>%
  mutate(Target2014 = ifelse(is.na(AStrat2014), 'Non-Target', Target2014)) %>%
  mutate(AStrat2014 = ifelse(AStrat2014 == 'Entiat IMW', paste('EntiatIMW', GeoRchIMW, sep = '_'), AStrat2014)) %>%
  filter(Target2014 == 'Target') %>%
  rename(Watershed = CHaMPshed)
  
# what strata do we have?
frame_strata = champ_frame_df %>%
  mutate(strata = paste(Watershed, AStrat2014, sep='_')) %>%
  select(Watershed, 
         strata) %>%
  distinct()

# how long is each strata?
chnk_strata_length = champ_frame_df %>%
  filter(!is.na(UseTypCHSP)) %>%
  mutate(strata = paste(Watershed, AStrat2014, sep='_')) %>%
  select(Watershed, matches("Strat"), FrameLeng) %>%
  group_by(Watershed, strata) %>%
  summarise(tot_length_km = sum(FrameLeng) / 1000) %>%
  ungroup() %>%
  mutate_at(vars(Watershed, strata), 
            list(as.factor)) %>%
  arrange(Watershed, strata)

sthd_strata_length = champ_frame_df %>%
  filter(!is.na(UseTypSTSU)) %>%
  mutate(strata = paste(Watershed, AStrat2014, sep='_')) %>%
  select(Watershed, matches("Strat"), FrameLeng) %>%
  group_by(Watershed, strata) %>%
  summarise(tot_length_km = sum(FrameLeng) / 1000) %>%
  bind_rows(tibble(Watershed = 'Asotin',
                   strata = paste('Asotin', c('CC', 'NF', 'SF'), sep = '_'),
                   tot_length_km = 12)) %>%
  ungroup() %>%
  mutate_at(vars(Watershed, strata), 
            list(as.factor)) %>%
  arrange(Watershed, strata)


# how many sites in each strata? and what is the length of each strata?
strata_tab = pred_hab_sites_chnk %>%
  select(Site, Watershed, matches('per_m')) %>%
  left_join(site_strata) %>%
  filter(strata != 'Entiat_Entiat IMW') %>%
  mutate_at(vars(Watershed),
            list(fct_drop)) %>%
  group_by(Watershed, strata) %>%
  summarise(n_sites = n_distinct(Site)) %>%
  ungroup() %>%
  full_join(chnk_strata_length) %>%
  mutate(n_sites = if_else(is.na(n_sites),
                           as.integer(0),
                           n_sites)) %>%
  # calculate the weight of each site in each strata
  mutate(site_weight = if_else(n_sites > 0,
                               tot_length_km / n_sites,
                               as.numeric(NA)))


strata_test = frame_strata %>%
  full_join(strata_tab) %>%
  mutate_at(vars(Watershed),
            list(fct_drop)) %>%
  mutate(n_sites = if_else(is.na(n_sites),
                           as.integer(0),
                           n_sites))

# # what frame strata don't have any sites in them?
# strata_test %>%
#   filter(n_sites == 0,
#          !is.na(tot_length_km)) %>%
#   arrange(Watershed, strata) %>%
#   as.data.frame()

# # what strata that we have sites for are not in the frame strata?
# strata_test %>%
#   filter(n_sites > 0,
#          (is.na(tot_length_km) |
#             tot_length_km == 0)) %>%
#   as.data.frame()

# how much of each watershed is not accounted for with current sites / strata?
strata_test %>%
  group_by(Watershed) %>%
  summarise_at(vars(tot_length_km),
               list(sum),
               na.rm = T) %>%
  left_join(strata_test %>%
              filter(n_sites == 0) %>%
              group_by(Watershed) %>%
              summarise_at(vars(missing_length = tot_length_km),
                           list(sum),
                           na.rm = T)) %>%
  mutate_at(vars(missing_length),
            list(~ if_else(is.na(.), 0, .))) %>%
  mutate(perc_missing = missing_length / tot_length_km) %>%
  mutate_at(vars(perc_missing),
            list(~ if_else(is.na(.), 0, .))) %>%
  arrange(desc(perc_missing)) %>%
  pander()



```

We then selected a subset of the globally available attributes (GAAs) to use as extrapolation covariates. This was based on minimizing the correlations between the covariates. We also acknowlegdged which prediction sites had GAAs outside the range of values for the CHaMP sites.

We use the log of predicted capacity wherever we could make those predictions as the response variable, and this group of selected GAAs as the covariates. We then fit a linear model that incorporated the survey design weights by using the `survey` pacakge (-@Lumley2014) in R. We fit two main models, one that included CHaMP watershed as a covariate, and one that did not. The former was used to make predictions within all CHaMP watersheds where we had QRF predictions of carrying capacity. The latter was used at all other master sample points. 

```{r}
data(gaa)

gaa_all = gaa %>%
  rename(Channel_Type = ChanlType)

# possible covariates from among the GAAs
gaa_covars = c('TRange', 'GDD', 'Precip', 'Elev_M', 'CHaMPsheds', 'NatPrin1', 'NatPrin2', 'DistPrin1', 'BFW_M', 'SrtCumDrn', 'StrmPwr', 'Slp_NHD_v1', 'Channel_Type', 'MAVELV', 'WIDE_BF', 'S2_02_11')

# what type of covariate is each GAA?
gaa_class = gaa_all %>%
  select(one_of(gaa_covars)) %>%
  as.list() %>%
  map_chr(.f = class)

# which ones are numeric?
gaa_num = names(gaa_class)[gaa_class %in% c('integer', 'numeric')]
# which ones are categorical?
gaa_catg = names(gaa_class)[gaa_class %in% c('factor', 'character')]

# compare range of covariates from model dataset and prediction dataset
range_comp = bind_rows(gaa_all %>%
                         filter(!Site %in% unique(pred_hab_sites$Site)) %>%
                         select(Site,
                                one_of(gaa_num)) %>%
                         gather(Metric, value, -Site) %>%
                         mutate(Source = 'non-DASH Sites'),
                       gaa_all %>%
                         filter(Site %in% unique(pred_hab_sites$Site)) %>%
                         select(Site,
                                one_of(gaa_num)) %>%
                         distinct() %>%
                         gather(Metric, value, -Site) %>%
                         mutate(Source = 'DASH Sites')) %>%
  mutate_at(vars(Source, Metric),
            list(as.factor))

range_max = range_comp %>%
  group_by(Metric, Source) %>%
  summarise_at(vars(value),
               tibble::lst(min, max),
               na.rm = T) %>%
  filter(Source == 'DASH Sites') %>%
  ungroup() %>%
  gather(type, value, -Metric, -Source)

covar_range_p = range_comp %>%
  ggplot(aes(x = Source,
             y = value,
             fill = Source)) +
  geom_boxplot() +
  facet_wrap(~ Metric,
             scales = 'free') +
  geom_hline(data = range_comp %>%
               group_by(Metric, Source) %>%
               summarise_at(vars(value),
                            tibble::lst(min, max),
                            na.rm = T) %>%
               filter(Source == 'DASH Sites') %>%
               ungroup() %>%
               gather(type, value, min, max),
             aes(yintercept = value),
             lty = 2,
             color = 'darkgray') +
  theme_minimal()

covar_range_p

# look at correlations among GAAs
library(corrr)
corr_df = gaa_all %>%
  select(one_of(gaa_num)) %>%
  correlate()

corr_df %>%
  shave(upper = T) %>%
  stretch() %>%
  arrange(desc(abs(r))) %>%
  filter(abs(r) >= 0.5)

gaa_corr_p1 = corr_df %>%
  rearrange(absolute = T) %>%
  shave(upper = T) %>%
  rplot(legend = T,
        colors = c('blue', 'white', 'indianred2'),
        print_cor = T)

# correlation matrix
corr_mat = gaa_all %>%
  select(one_of(gaa_num)) %>%
  cor(use = 'pairwise.complete.obs')


library(ggcorrplot)
gaa_corr_p2 = ggcorrplot(corr_mat,
           lab = T,
           lab_size = 2,
           hc.order = T,
           colors = c('#2166AC', 'white', '#B2182B'),
           tl.cex = 8,
           type = 'full')
```


```{r}
# reduce number of covariates bases on correlations
gaa_covars = c('TRange', 
               # 'GDD', 
               # 'Precip',
               'Elev_M', 
               'CHaMPsheds', 
               'NatPrin1', 
               # 'NatPrin2', 
               'DistPrin1', 
               # 'BFW_M', 
               'SrtCumDrn', 
               'StrmPwr',
               'Slp_NHD_v1', 
               'Channel_Type', 
               # 'MAVELV', 
               'WIDE_BF',
               'S2_02_11')

# what type of covariate is each GAA?
gaa_class = gaa_all %>%
  select(one_of(gaa_covars)) %>%
  as.list() %>%
  map_chr(.f = class)

# which ones are numeric?
gaa_num = names(gaa_class)[gaa_class %in% c('integer', 'numeric')]
# which ones are categorical?
gaa_catg = names(gaa_class)[gaa_class %in% c('factor', 'character')]

# Center the covariates
# filter out sites with covariates outside range of covariates used to fit extrapolation model
out_range_sites = gaa_all %>%
  # filter out a few areas
  filter(!HUC6NmNRCS %in% c('Upper Sacramento', 'Southern Oregon Coastal', 'Puget Sound', 'Northern California Coastal', 'Oregon Closed Basins')) %>%
  # don't use AEM sites in model
  filter(!grepl('^AEM', Site)) %>%
  select(one_of(gaa_num), Site) %>%
  gather(Metric, value, -Site) %>%
  left_join(select(range_max, -Source) %>%
              spread(type, value)) %>%
  group_by(Metric) %>%
  filter(value > max |
           value < min) %>%
  ungroup() %>%
  pull(Site) %>%
  unique()

# center covariates
gaa_summ = inner_join(pred_hab_sites %>%
                          select(Site) %>%
                          distinct(),
                        gaa_all %>%
                          select(Site, one_of(gaa_num))) %>%
  gather(GAA, value, -Site) %>%
  group_by(GAA) %>%
  summarise(metric_mean = mean(value, na.rm=T),
            metric_sd = sd(value, na.rm=T)) %>%
  ungroup()

# extrapolation model data set, with normalized covariates
mod_data = inner_join(pred_hab_sites_chnk %>%
                        select(Site:avg_aug_temp,
                               matches('per_m'),
                               -one_of(gaa_num)),
                      gaa_all %>%
                        select(Site, one_of(gaa_num))) %>%
  gather(GAA, value, one_of(gaa_num)) %>%
  left_join(gaa_summ) %>%
  mutate(norm_value = (value - metric_mean) / metric_sd) %>%
  select(-(value:metric_sd)) %>%
  spread(GAA, norm_value) %>%
  left_join(gaa_all %>%
              select(Site, one_of(gaa_catg)))

mod_data %<>%
  bind_cols(mod_data %>%
              is.na() %>%
              as_tibble() %>%
              select(one_of(gaa_covars)) %>%
              transmute(n_NA = rowSums(.))) %>%
  filter(n_NA == 0) %>%
  mutate_at(vars(Watershed, CHaMPsheds, Channel_Type),
            list(fct_drop)) 


# where do we want to make extrapolation predictions?
gaa_pred = gaa_all %>%
  # filter out a few areas
  filter(!HUC6NmNRCS %in% c('Upper Sacramento', 'Southern Oregon Coastal', 'Puget Sound', 'Northern California Coastal', 'Oregon Closed Basins')) %>%
  # don't use AEM sites in model
  filter(!grepl('^AEM', Site)) %>%
  # don't use non-GRTS sites
  filter(SiteID_alt != 'NonGRTSSite' | is.na(SiteID_alt)) %>%
  filter(!grepl('mega', Site, ignore.case = T)) %>%
  # note which sites have GAAs outside range of CHaMP sites GAAs
  mutate(inCovarRange = ifelse(Site %in% out_range_sites, F, T)) %>%
  select(Site, one_of(gaa_covars), Lon, Lat, inCovarRange, HUC6NmNRCS, HUC8NmNRCS, HUC10NmNRC, HUC12NmNRC, chnk) %>%
  gather(GAA, value, one_of(gaa_num)) %>%
  left_join(gaa_summ) %>%
  mutate(norm_value = (value - metric_mean) / metric_sd) %>%
  select(-(value:metric_sd)) %>%
  spread(GAA, norm_value)

```

```{r with_model_weights, eval = T}
# calculate adjusted weights for all predicted QRF capacity sites
mod_data_weights = mod_data %>%
  left_join(site_strata) %>%
  left_join(strata_tab) %>%
  filter(!is.na(site_weight)) %>%
  group_by(Watershed) %>%
  mutate(sum_weights = sum(site_weight)) %>%
  ungroup() %>%
  mutate(adj_weight = site_weight / sum_weights)

#-------------------------------------------------------------
# Set up the survey design.

# getOption('survey.lonely.psu')
# this will prevent strata with only 1 site from contributing to the variance
# options(survey.lonely.psu = 'certainty')
# this centers strata with only 1 site to the sample grand mean; this is conservative
options(survey.lonely.psu = 'adjust')

# extrapolation model formula
full_form = as.formula(paste('log(qrf_cap) ~ -1 + (', paste(gaa_covars, collapse = ' + '), ')'))

# fit various models
model_svy_df = mod_data_weights %>%
  gather(response, qrf_cap, matches('per_m')) %>%
  select(-(n_sites:sum_weights)) %>%
  group_by(response) %>%
  nest() %>%
  mutate(design = map(data,
                      .f = function(x) {
                        svydesign(id = ~ 1,
                                  data = x,
                                  strata = ~ Watershed,
                                  # strata = ~ strata,
                                  weights = ~ adj_weight)
                      })) %>%
  mutate(mod_full = map(design,
                        .f = function(x) {
                          svyglm(full_form,
                                 design = x)
                        }),
         mod_no_champ = map(design,
                            .f = function(x) {
                              svyglm(update(full_form, .~ . -CHaMPsheds),
                                     design = x)
                            }))
```


```{r make-predictions}
# predictions within CHaMP watersheds, using the model that includes CHaMPsheds as a covariate
y = gaa_pred %>%
  filter(CHaMPsheds %in% unique(model_svy_df$data[[1]]$Watershed)) %>%
  select(Site, CHaMPsheds, one_of(gaa_covars)) %>%
  na.omit() %>%
  left_join(gaa_pred) %>%
  mutate_at(vars(Channel_Type),
            list(as.factor))

per_m_preds = predict(model_svy_df$mod_full[[1]],
                      newdata = y,
                      se = T,
                      type = 'response')

per_m2_preds = predict(model_svy_df$mod_full[[2]],
                      newdata = y,
                      se = T,
                      type = 'response')

y %<>%
  bind_cols(per_m_preds %>%
              as_tibble() %>%
              rename(chnk_per_m = response,
                     chnk_per_m_se = SE)) %>%
  bind_cols(per_m2_preds %>%
              as_tibble() %>%
              rename(chnk_per_m2 = response,
                     chnk_per_m2_se = SE)) %>%
  mutate_at(vars(matches('per_m')),
            list(exp))

rm(per_m_preds, per_m2_preds)

# predictions at all points, using the model without CHaMPsheds as a covariate
z = gaa_pred %>%
  select(Site, one_of(gaa_covars), -CHaMPsheds) %>%
  na.omit() %>%
  left_join(gaa_pred) %>%
  mutate_at(vars(Channel_Type),
            list(as.factor))

per_m_preds = predict(model_svy_df$mod_no_champ[[1]],
                      newdata = z,
                      se = T,
                      type = 'response')

per_m2_preds = predict(model_svy_df$mod_no_champ[[2]],
                      newdata = z,
                      se = T,
                      type = 'response')

z %<>%
  bind_cols(per_m_preds %>%
              as_tibble() %>%
              rename(chnk_per_m = response,
                     chnk_per_m_se = SE)) %>%
  bind_cols(per_m2_preds %>%
              as_tibble() %>%
              rename(chnk_per_m2 = response,
                     chnk_per_m2_se = SE)) %>%
  mutate_at(vars(matches('per_m')),
            list(exp))

rm(per_m_preds, per_m2_preds)

all_preds = y %>%
  mutate(model = 'CHaMP') %>%
  bind_rows(z %>%
              mutate(model = 'non-CHaMP'))

# quick comparison of capacity predicitons with both models
comp_pred_p = all_preds %>%
  filter(Site %in% Site[duplicated(Site)]) %>%
  select(Site, CHaMPsheds, Channel_Type, model, chnk_per_m, chnk_per_m2) %>%
  arrange(CHaMPsheds, Site, model) %>%
  gather(dens_type, cap, starts_with('chnk_per_m')) %>%
  spread(model, cap) %>%
  ggplot(aes(x = CHaMP,
             y = `non-CHaMP`)) +
  geom_point() +
  geom_abline(linetype = 2,
              color = 'red') +
  facet_wrap(~ CHaMPsheds + dens_type,
             scales = 'free')

# for sites in CHaMP watersheds, use predicions from CHaMP extrapolation model
all_preds %<>%
  filter((Site %in% Site[duplicated(Site)] & model == 'CHaMP') |
           !Site %in% Site[duplicated(Site)])

# for CHaMP sites, use direct QRF esimates, not extrapolation ones
all_preds %<>%
  left_join(mod_data %>%
              select(Site, 
                     qrf_chnk_per_m = chnk_per_m,
                     qrf_chnk_per_m2 = chnk_per_m2)) %>%
  mutate(chnk_per_m = if_else(!is.na(qrf_chnk_per_m),
                              qrf_chnk_per_m,
                              chnk_per_m),
         chnk_per_m_se = if_else(!is.na(qrf_chnk_per_m),
                                 as.numeric(NA),
                                 chnk_per_m_se),
         chnk_per_m2 = if_else(!is.na(qrf_chnk_per_m2),
                               qrf_chnk_per_m2,
                               chnk_per_m2),
         chnk_per_m2_se = if_else(!is.na(qrf_chnk_per_m2),
                                  as.numeric(NA),
                                  chnk_per_m2_se)) %>%
  select(-starts_with('qrf'))


```

```{r save-extrapolation}
# usethis::use_data(gaa_covars,
#                   mod_data_weights,
#                   # model_svy_df,
#                   all_preds,
#                   version = 2,
#                   overwrite = T)

write_rds(gaa_covars,
          "../data/derived_data/gaa_covars.rds")
write_rds(mod_data_weights,
          "../data/derived_data/mod_data_weights.rds")
write_rds(model_svy_df,
          "../data/derived_data/model_svy_df.rds")
write_rds(all_preds,
          "../data/derived_data/all_preds.rds")


```

# Validation

We validated our estimates by comparing them with selected areas that had spawner-recruit data available.

```{r}
library(sf)
data("chnk_domain")
# read in trap polygons
trap_poly = read_sf('../data/derived_data/trap_regions.shp') %>%
  # st_transform(crs = 5070)
  st_transform(st_crs(chnk_domain)) %>%
  select(-Id)

capacity_pts = all_preds %>%
  st_as_sf(coords = c('Lon', 'Lat'),
           crs = 4326) %>%
  st_transform(st_crs(chnk_domain))

qrf_caps = trap_poly %>%
  split(list(.$trap_name)) %>%
  map_df(.id = 'Trap',
         .f = calc_watershed_cap,
         spp_range = chnk_domain,
         capacity_pts = capacity_pts,
         capacity_name = "chnk_per_m",
         capacity_se_name = 'chnk_per_m_se') %>%
  mutate(tot_cap_cv = tot_cap_se / tot_cap) %>%
  mutate(Trap = recode(Trap,
                       'Minam R.' = 'Minam River',
                       'Entiat R.' = 'Entiat River',
                       'Crooked Fork Creek' = 'Crooked Fork',
                       'Upper Grande Ronde R.' ='Upper Grande Ronde River',
                       'Toucannon R.' = 'Tucannon River',
                       'Pahsimeroi' = 'Pahsimeroi River',
                       'Hood R.' = 'Hood River',
                       'John Day Middle Fork' = 'Middle Fork John Day',
                       'Upper Salmon River' = 'Upper Salmon River Chinook',
                       'John Day Upper Mainstem' = 'Upper Mainstem John Day',
                       'East Fork Salmon' = 'East Fork Salmon River',
                       'Lostine R.' = 'Lostine River'))

library(FSA)
bh1 <- srFuns('BevertonHolt',
              param = 3)

data("spawn_rec_data")
# fit Beverton-Holt curve with capacity fixed at QRF capacity prediction
qrf_params = spawn_recr_data %>%
  group_by(Population) %>%
  nest() %>%
  left_join(qrf_caps,
            by = c('Population' = 'Trap')) %>%
  filter(!is.na(tot_cap)) %>%
  mutate(form = 'QRF',
         inits = map2(.x = data,
                      .y = tot_cap,
                     .f = function(x, y) {
                       inits = srStarts(Parr ~ Spawners,
                                        data = x,
                                        type = 'BevertonHolt',
                                        param = 3)

                       inits$a = if_else(inits$a < 0, 
                                         2e-3, 
                                         inits$a)
                       inits$b = 1 / y
                       return(inits)
                       }),
         mod_fit = map2(.x = data,
                        .y = inits,
                        .f = function(x, y) {
                          fit = try(nls(log(Parr) ~ log(bh1(Spawners, a, b)),
                                        data = x,
                                        start = y,
                                        algorithm = 'port',
                                        lower = c(0, y['b']),
                                        upper = c(Inf, y['b'])))
                        }),
         coefs = map(mod_fit,
                     .f = function(x) {
                       if(class(x) == 'try-error') {
                         return(as.numeric(NA))
                       } else coef(x)
                       }),
         preds = map2(.x = data,
                      .y = mod_fit,
                      .f = function(x, y) {
                        if(class(y) != 'try-error') {
                          tibble(Spawners = 1:max(x$Spawners)) %>%
                            mutate(Parr = predict(y,
                                                  newdata = .),
                                   Parr = exp(Parr))
                        } else return(tibble(Spawners = NA,
                                             Parr = NA))
                      })) %>%
  rename(est = tot_cap,
         se = tot_cap_se) %>%
  select(-c(area:tot_length, tot_cap_cv)) %>%
  mutate(cv = se / est)


# combine with other spawner recruit estimates
data("spawn_rec_params")

spawn_rec_params %<>%
  bind_rows(qrf_params)

# save complete table
write_rds(spawn_rec_params,
          path = "../data/derived_data/spawn_rec_params.rds")

```

